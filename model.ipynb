{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Use Case: ZAF043 Crowd Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 - Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Frames Shape: (6,)\n",
      "Training Labels Shape: (6,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_training_data(directory):\n",
    "    # Initialize lists for frames and labels\n",
    "    frames = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over the files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Get the full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        # Load the frame using OpenCV\n",
    "        frame = cv2.imread(file_path)\n",
    "\n",
    "        # Preprocess the frame if needed\n",
    "\n",
    "        # Append the frame to the frames list\n",
    "        frames.append(frame)\n",
    "\n",
    "        # Extract the label from the filename (assuming the filename contains the label information)\n",
    "        label = filename.split(\"_\")[0]  # Extract the label from the filename based on the naming convention\n",
    "        labels.append(label)\n",
    "\n",
    "    # Convert frames and labels to numpy arrays\n",
    "    frames = np.array(frames)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return frames, labels\n",
    "\n",
    "# Directory path containing the training data\n",
    "training_data_directory = \"D:\\\\Data Set\\\\Crowd Anomaly Detection\\\\UCSD_Anomaly_Dataset\"\n",
    "\n",
    "# Load training data\n",
    "training_frames, training_labels = load_training_data(training_data_directory)\n",
    "\n",
    "# Print the shape of the training frames and labels\n",
    "print(\"Training Frames Shape:\", training_frames.shape)\n",
    "print(\"Training Labels Shape:\", training_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2 - Data Preprocessing, Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(frames):\n",
    "    features = []\n",
    "    \n",
    "    # Convert frames to grayscale\n",
    "    gray_frames = [cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) for frame in frames]\n",
    "    \n",
    "    # Calculate optical flow for consecutive frames\n",
    "    for i in range(len(frames) - 1):\n",
    "        prev_frame = gray_frames[i]\n",
    "        next_frame = gray_frames[i + 1]\n",
    "        \n",
    "        # Compute optical flow using Lucas-Kanade method\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_frame, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        \n",
    "        # Extract relevant features from the optical flow\n",
    "        # Example: Compute the mean and standard deviation of the flow vectors\n",
    "        mean_flow = np.mean(flow)\n",
    "        std_flow = np.std(flow)\n",
    "        \n",
    "        # Append the extracted features to the list\n",
    "        features.append([mean_flow, std_flow])\n",
    "    \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3 - Models\n",
    "In this code, the train_mdt_model() function takes features (the extracted features) and num_components (the number of components in the MDT model) as inputs. It initializes a Gaussian Mixture Model (GMM) with n_components=num_components and fits it to the features using the fit() method. The trained model is then returned.\n",
    "The scikit-learn library installed (pip install scikit-learn) before running this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def train_mdt_model(features, num_components):\n",
    "    # Convert the list of features into a 3D numpy array\n",
    "    features_array = np.array(features)\n",
    "    \n",
    "    # Get the shape of the array\n",
    "    num_samples, num_frames, num_features = features_array.shape\n",
    "    \n",
    "    # Reshape the array to have two dimensions\n",
    "    features_2d = features_array.reshape(num_samples * num_frames, num_features)\n",
    "    \n",
    "    # Train a Gaussian Mixture Model (GMM) on the extracted features\n",
    "    gmm = GaussianMixture(n_components=num_components)\n",
    "    gmm.fit(features_2d)\n",
    "    \n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_score(observation, model):\n",
    "    # Compute the anomaly score by comparing the observation with the learned model\n",
    "    score = -model.score_samples(observation.reshape(1, -1))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames: 7000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tifffile\n",
    "\n",
    "# Specify the directory path containing the frames\n",
    "directories =[\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train001\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train002\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train003\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train004\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train005\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train006\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train007\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train008\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train009\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train010\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train011\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train012\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train013\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train014\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train015\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train016\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train017\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train018\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train019\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train020\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train020\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train021\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train022\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train023\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train024\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train025\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train026\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train027\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train028\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train029\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train030\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train031\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train032\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train033\",\n",
    "    r\"D:\\Data Set\\Crowd Anomaly Detection\\UCSD_Anomaly_Dataset\\UCSDped1\\Train\\Train034\"\n",
    "    \n",
    "]\n",
    "# Initialize an empty list to store the frames\n",
    "frames = []\n",
    "\n",
    "# Iterate over the directories\n",
    "for directory_path in directories:\n",
    "    # Iterate over the files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Check if the file is a .tif image\n",
    "        if filename.endswith(\".tif\"):\n",
    "            # Read the frame using tifffile\n",
    "            frame = tifffile.imread(file_path)\n",
    "\n",
    "            # Append the frame to the list\n",
    "            frames.append(frame)\n",
    "\n",
    "# Print the number of frames loaded\n",
    "print(\"Number of frames:\", len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "num_components = 10  # Number of components in the MDT model\n",
    "features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(frames):\n",
    "    features = []\n",
    "\n",
    "    # Calculate optical flow for consecutive frames\n",
    "    for i in range(len(frames) - 1):\n",
    "        frame1 = frames[i]\n",
    "        frame2 = frames[i + 1]\n",
    "\n",
    "        # Calculate optical flow\n",
    "        flow = cv2.calcOpticalFlowFarneback(frame1, frame2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        # Flatten the flow matrix and append it to the features\n",
    "        flow_flat = flow.reshape(-1)\n",
    "        features.append(flow_flat)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the features\n",
    "features = []\n",
    "\n",
    "# Iterate over the frames and extract features\n",
    "for frame in frames:\n",
    "    # Preprocess frame if needed\n",
    "    # Extract features from the frame\n",
    "    frame_features = extract_features(frame)\n",
    "    features.append(frame_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to numpy array\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert the list of features into a 3D numpy array\n",
    "features_array = np.array(features)\n",
    "\n",
    "# Get the shape of the array\n",
    "num_samples, num_frames, num_features = features_array.shape\n",
    "\n",
    "# Reshape the array to have two dimensions\n",
    "features_2d = features_array.reshape(num_samples * num_frames, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train MDT model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mdt_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mdt_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_components\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mtrain_mdt_model\u001b[1;34m(features, num_components)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train a Gaussian Mixture Model (GMM) on the extracted features\u001b[39;00m\n\u001b[0;32m     15\u001b[0m gmm \u001b[38;5;241m=\u001b[39m GaussianMixture(n_components\u001b[38;5;241m=\u001b[39mnum_components)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mgmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gmm\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\mixture\\_base.py:200\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    The method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m        The fitted mixture.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\mixture\\_base.py:264\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    262\u001b[0m     prev_lower_bound \u001b[38;5;241m=\u001b[39m lower_bound\n\u001b[1;32m--> 264\u001b[0m     log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_m_step(X, log_resp)\n\u001b[0;32m    266\u001b[0m     lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_lower_bound(log_resp, log_prob_norm)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\mixture\\_base.py:321\u001b[0m, in \u001b[0;36mBaseMixture._e_step\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_e_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;124;03m\"\"\"E step.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m        the point of each sample in X.\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 321\u001b[0m     log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_log_prob_resp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(log_prob_norm), log_resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\mixture\\_base.py:541\u001b[0m, in \u001b[0;36mBaseMixture._estimate_log_prob_resp\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_estimate_log_prob_resp\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;124;03m\"\"\"Estimate log probabilities and responsibilities for each sample.\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \n\u001b[0;32m    525\u001b[0m \u001b[38;5;124;03m    Compute the log probabilities, weighted log probabilities per\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m        logarithm of the responsibilities\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 541\u001b[0m     weighted_log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_weighted_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m     log_prob_norm \u001b[38;5;241m=\u001b[39m logsumexp(weighted_log_prob, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;66;03m# ignore underflow\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\mixture\\_base.py:494\u001b[0m, in \u001b[0;36mBaseMixture._estimate_weighted_log_prob\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_estimate_weighted_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;124;03m\"\"\"Estimate the weighted log-probabilities, log P(X | Z) + log weights.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;124;03m    weighted_log_prob : array, shape (n_samples, n_component)\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_log_weights()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:760\u001b[0m, in \u001b[0;36mGaussianMixture._estimate_log_prob\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_estimate_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_estimate_log_gaussian_prob\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeans_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecisions_cholesky_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariance_type\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:427\u001b[0m, in \u001b[0;36m_estimate_log_gaussian_prob\u001b[1;34m(X, means, precisions_chol, covariance_type)\u001b[0m\n\u001b[0;32m    425\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((n_samples, n_components))\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, (mu, prec_chol) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(means, precisions_chol)):\n\u001b[1;32m--> 427\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprec_chol\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(mu, prec_chol)\n\u001b[0;32m    428\u001b[0m         log_prob[:, k] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39msquare(y), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m covariance_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtied\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train MDT model\n",
    "mdt_model = train_mdt_model(features, num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tsfresh\n",
    "import pandas as pd\n",
    "\n",
    "#Read the image\n",
    "frame = cv2.imread('D:/Data Set/Crowd Anomaly Detection/UCSD_Anomaly_Dataset/UCSDped1/Test/Test034/023.tif')\n",
    "\n",
    "#Reshape the image to 2 dimensions\n",
    "frame_2d = frame.reshape((frame.shape[0] * frame.shape[1], 3))\n",
    "\n",
    "#Create a Pandas DataFrame\n",
    "frame_df = pd.DataFrame(frame_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(frame):\n",
    "    new_features = extract_features([new_frame])\n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the new frame using OpenCV\n",
    "new_frame = cv2.imread(\"D:/Data Set/Crowd Anomaly Detection/UCSD_Anomaly_Dataset/UCSDped1/Test/Test034/023.tif\")\n",
    "\n",
    "# Check if the new frame is loaded successfully\n",
    "if new_frame is None:\n",
    "    print(\"Failed to load the new frame.\")\n",
    "    exit()\n",
    "\n",
    "# Extract features from the new frame\n",
    "new_features = extract_features([new_frame])  # Pass the new frame as a list to the extract_features function\n",
    "\n",
    "# Check if the features are extracted successfully\n",
    "if len(new_features) == 0:\n",
    "    print(\"Failed to extract features from the new frame.\")\n",
    "    exit()\n",
    "\n",
    "# Convert the features to a numpy array\n",
    "new_features = np.array(new_features)\n",
    "\n",
    "# Reshape the features for prediction\n",
    "new_observation = new_features.reshape(1, -1)\n",
    "\n",
    "# Compute anomaly score for the new observation\n",
    "anomaly_score = compute_anomaly_score(new_observation, mdt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anomaly Score:\", anomaly_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### *Conclusion:* \n",
    "Based on the provided code and information, the recommended model for anomaly detection in crowd behavior is the MDT (Motion Density Texture) model trained using Gaussian Mixture Model (GMM). This model utilizes optical flow features extracted from consecutive frames to learn patterns and detect anomalies in crowd behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4 - Saving the model\n",
    "Save the DS best model in the Jupyter notebook `model.ipynb` in the following formats:\n",
    "\n",
    "- `network.save('model.h5')` #keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mdt_model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
